\documentclass{article} %A4
\usepackage[a4paper,left=1.9cm, right=2.1cm,top = 1.2cm,bottom=2.3cm]{geometry}
\usepackage[utf8]{inputenc}%Umlaute
\usepackage[ngerman]{babel} %Texttrennung
\usepackage{graphicx}	%Grafiken
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{url}
\usepackage{listings}
 \usepackage{color}
\usepackage{hyperref}
\usepackage{framed}
\usepackage{algpseudocode}
\usepackage{tikz}
\usepackage{sgame}
\usepackage{multicol}
\usepackage[labelformat=empty]{caption}

\title{Zusammenfassung - Multi-Agenten-Systeme}
\author{
	Andreas Ruscheinski,
	Marc Eric Meier, 
	CD
}
\begin{document}
\maketitle
\begin{framed}Korrektheit und Vollständigkeit der Informationen sind nicht gewährleistet.
Macht euch eigene Notizen oder ergänzt/korrigiert meine Ausführungen!
\end{framed}
\setcounter{tocdepth}{1}
\tableofcontents

\section{Einführung}
	\subsection{Definition}
	\begin{itemize}
		\item Ein Agent ist ein Computer System, welches \textbf{selbstständig} Aktionen im Interesse des Benutzers ausführen kann.
		\item Ein Agent \textbf{befindet} sich in einer \textbf{dynamischen Umgebung}, mit welcher er interagiert.
		\item Ein Multi-Agenten-System besteht aus \textbf{mehreren Agenten}, welche \textbf{miteinander interagieren}.
		\item In einem Multi-Agenten-System ist es für eine  \textbf{erfolgreiche Interaktion} notwendig, dass die Agenten miteinander \textbf{kooperieren}, \textbf{verhandeln} und \textbf{sich abstimmen} können.
	\end{itemize}
	\subsection{Eigenschaften}
	\begin{itemize}
		\item Jeder Agent hat \textbf{keine vollständigen Informationen} über die Umgebung.
		\item Es gibt \textbf{keine globale Kontrolle} der Agenten.
		\item Die Daten sind \textbf{dezentralisiert}.
		\item Die Berechnung erfolgt \textbf{asynchron}.
	\end{itemize}
	\subsection{Gründe für den Einsatz von MAS}
	\begin{itemize}
		\item Ein Problem kann nicht zentralisiert gelöst werden, da die \textbf{Ressourcen limitiert} sind.\\
		$\rightarrow$\textit{Verteilte Berechnungen}
		\item \textbf{Reduktion der Ausfall-Wahrscheinlichkeit} gegenüber einem zentralisierten System
		\item \textbf{Gewährleistung der Interkonnektion und Interoperation} von verschiedenen Systemen\\
		$\rightarrow$\textit{Migration von veralteter Software}
		\item Lösung von Problemen, die sich mit einer \textbf{ Menge von autonomen Komponenten beschäftigen}\\
		$\rightarrow$\textit{Luftfahrkontrolle, Terminkalender}
	\end{itemize}
	\subsection{Konkrete Anwendungsgebiete}
	\begin{itemize}
		\item Cloud-Management
		\item Ubiquitous Computing
		\item Grid-Software
		\item Spiele
		\item Verschiedene Gebiete der Industrie (Car-Assembly, Factory Management)
		\item Simulation
	\end{itemize}
	MAS befindet sich in der Schnittmenge aus KI, Spieltheorie, Sozialforschung und Verteilten Systemen.
\section{Rolle der Logik in MAS}
	\subsection{Gründe für Logik}
	\begin{itemize}
		\item Wissensbasis + Aktionen mit Voraussetzung und Auswirkung $\rightarrow$ Plan für Lösung des Problems
		\item Verwaltung von Annahmen
		\item Logik ist ein Framework für das Verstehen von Systemen
		\item Verifikation, Ausführungsspezifikation, Planung
		\item Agenten als Theorembeweiser
	\end{itemize}
	\subsection{Logik-basierende Architektur}
	\begin{itemize}
		\item Grundidee: Aufstellen einer Regelmenge zur Beschreibung der besten Aktion bei einem gegebenem Zustand
		\item Bestandteile:
		\begin{itemize}
			\item $p$: Eine Theorie (Menge von Regeln)
			\item $\Delta$: Datenbank mit dem aktuellen Zustand der Welt
			\item $A$: Eine Menge von Aktionen, die ein Agent ausführen kann
			\item $\Delta\vdash_{p}\phi$: d.h. $\phi$ kann aus der $\Delta$ unter der Verwendung von $p$ abgeleitet werden, mit $\phi=$Do(a) können wir aus dem aktuellen Zustand der Welt auf die bestmögliche Aktion logisch schließen.
		\end{itemize}
		 \item Algorithmus-Bestandteile (unabhängig von einer verwendeter Logik)
		 \begin{itemize}
		 	\item \textbf{see(s,p)}, generiert Beobachtung aus der aktuellen Welt $s$ (schwer siehe Vorlesung Mustererkennung und Kontextanalyse)
		 	\item \textbf{next($\Delta$,p)}, Update der Datenbank mit der Beobachtung
		 	\item \textbf{action($\Delta$)}, ermittelt die auszuführende Aktion aus der Datenbank, entweder ist die Aktion direkt beschrieben oder kann aus den Regeln abgeleitet werden.
		 \end{itemize}
		 \item Algorithmus - Ermittelung einer Aktion aus einer Wissensdatenbank:
		 \begin{enumerate}
		 	\item Überprüfe für jede Aktion a, ob  $\Delta\vdash_{p}Do(a)$ gilt (d.h. a kann direkt abgeleitet werden)
		 	\item Falls ja: Gib a zurück; sonst
		 	\item Überprüfe für jede Aktion a, ob $\Delta\not{\vdash}_{p}\neg Do(a)$ gilt (d.h a ist nicht explizit ausgeschlossen)
		 	\item Falls ja: Gib a zurück, sonst gib NULL zurück (keine Aktion gefunden)
		 \end{enumerate}
	\end{itemize}
	\subsection{Modale Logik}
	\begin{itemize}
		\item Erlaubt Ausdrücke wie: wahrscheinlich wahr, geglaubt wahr, wahr in der Zukunft usw.
		\item Kann genutzt werden, um Informationen für die Agenten abzuleiten und über das Wissen von Agenten zu schließen
		\item Syntax:
		\begin{itemize}
			\item Prädikatenlogik mit Erweiterung
			\item Prop: Eine Menge von atomaren Formeln
			\item Wenn $p,q \in Prob$, dann sind auch $\neg p, p \& q, \diamond p,\square p$ Formeln
			\item $\diamond$ p: möglicherweise p, manchmal p 
			\item $\square$p: immer p, notwendigerweise p
		\end{itemize}
		\item Semantik:
		\begin{itemize}
			\item Kripke-Struktur: $\mathcal{K} = <W,R,\mu>$
			\begin{description}
				\item[$W$] eine Menge von Welten
				\item[$R$] eine Menge von binären Relationen, die den Übergang zwischen den Welten  beschreiben
				\item[$\mu$] Abbildungsfunktion, die jeder Welt Eigenschaften zuordnet ($\mu : W \rightarrow 2^{Prop}$)	
			\end{description}
				\item Rahmen: $<W,R>$
				\begin{itemize}
					\item  $W$ und $R$ definiert, wie in Kripke Struktur
					\item $R \vdash F$, falls gilt: $\forall \mathcal{K},s: \mathcal{K},s \vDash F$ 
					\item Die \textbf{Korrespondenztheorie}: Zusammenhang zwischen Axiomensystemen und Rahmen					
				\end{itemize}
			\item Definition von $\diamond$ und $\square$ Operator auf Basis von Erreichbarkeit der Welten in einer Kripke-Struktur
			\item $\square p$: p ist wahr in allen Welten, welche von der aktuellen Welt erreichbar sind
			\item $<M,w>\vDash\square p$, wenn für alle $w' \in W$ gilt: wenn $(w,w') \in R$ denn $<M,w'>\vDash p$
			\item $\diamond p$: p ist wahr, wenn mindestens eine Welt erreichbar ist, in welcher p wahr ist
			\item $<M,w>\vDash\diamond p$, wenn es ein $w' \in W$ existiert mit:  wenn $(w,w') \in R$ denn $<M,w'>\vDash p$\\
			
			\item Für R muss zusätzlich gelten:
			\begin{description}
				\item[reflexiv] für jedes $x \in W$ gilt $R(x,x)$, d.h. x ist von x aus erreichbar.
				\item[transitiv] für jedes $x,y,z \in W$ gilt $R(x,y) \wedge R(y,z) \implies R(x,z)$, d.h. wenn man von x nach y und von y nach z gehen kann, kann man auch von x nach z gehen.
				\item[seriell] für jedes $x \in W$ existiert ein $y$, so dass gilt $R(x,y)$, d.h. jede Welt ist mit einer anderen Welt in Relation.
				\item[euklidisch] wenn für jedes $x,y,z \in W$ mit $R(x,y)$ und $R(x,z)$, dann gilt auch $R(y,z)$, d.h. wenn man von x nach y und von x nach z gehen kann, dann kann man auch von y nach z gehen.\\
				
			\end{description}
			\begin{table}[h]
				\centering
				\label{my-label}
				\begin{tabular}{llll}
					\hline
					Name & Axiom                          & Condition on R & First-order characterization \\ \hline
					T    & $\square\phi \Rightarrow \phi$ & Reflexive      & $\forall w\in W:(w,w)\in R$  \\
					D & $\square\phi \Rightarrow \diamond\phi$ &        Serial        &$\forall w \in W : \exists w' \in W: (w,w')\in R$\\
					4&$\square\phi\Rightarrow\square\square\phi$ &            Transitive    & $\forall w,w',w''\in W: (w,w')\in R \wedge (w',w'')\in R \Rightarrow (w,w'') \in R$ \\
					5 &$\diamond\phi\Rightarrow\square\diamond\phi$&     Euclidiean           & $\forall w,w',w'' \in W : (w,w') \in R \wedge (w,w'') \in R \Rightarrow (w',w'') \in R$\\ \hline
				\end{tabular}
		\caption{Korrespondenztheorie}
			\end{table}
			\item Axiome (Beschreiben Eigenschaften für $R$):
			\begin{description}
				\item[$\square p \Rightarrow p$] Wann immer $p$ wahr ist, folgt daraus, dass aktuell p gilt (\textbf{Reflexiv})
				\item[$\square p \Rightarrow \diamond p$]Wann immer $p$ wahr ist, ist p auch in mindestens einer Welt wahr (\textbf{Seriell})
				\item[$\square p \Rightarrow \square \square p$]Wann immer $p$ wahr ist, ist p auch immer wahr, wenn wir einen Übergang machen (\textbf{transitiv})
				\item[$\diamond p \Rightarrow \square \diamond p$] Wenn $p$ in mindestens einer Welt wahr ist, ist p für immer wahr, wenn wir diese Welt erreicht haben (\textbf{euklidisch})\\
				
			\end{description}
			
			\item Anwendung der Modal Logik auf Agenten durch Einführung von Indizes, welche entsprechend für Agent gelten
			\item Operator - Knowledge: $K_ip$ bedeutet, dass Agent i $p$ weiß
			\item Der Modale Operator $\square_i$ wird zu $K_i$\\
			
			\item Axiome und Agenten:
			\begin{description}
				\item[$K_{i}p \Rightarrow p$] Wenn Agent glaubt, dass p wahr ist, ist p auch in Wirklichkeit wahr
				\item[$K_{i}p \Rightarrow \neg K_{i}\neg p$] Wenn Agent glaubt, dass p wahr ist, glaub er nicht die Negation
				\item[$K_{i}p \Rightarrow K_{i}K_{i}^p$] Wenn Agent glaubt, dass p wahr ist, weiß er selbst, dass er p glaubt
				\item[$\neg K_{i}\neg p \Rightarrow K_{i}\neg K_{i}\neg p$] Wenn der Agent nicht p glaubt, weiß er dass er nicht p glaubt.\\
				
			\end{description}
			\item Es gilt: $<M,w> \vDash K_i p$, genau dann wenn $\forall w' \in W$ gilt: wenn $(w,w') \in R_i$, dann $<M,w'> \vDash p$ (vgl. Definition von $\square$)
			\item Probleme:
			\begin{itemize}
				\item Agent glaubt alles was wahr ist incl. Tautologien: wenn $\vDash p$ denn $K_i p$
				\item Agent weiß alle Schlussfolgerungen des eigenen Wissens: $\vDash p \rightarrow q$ denn $\vDash K_i p \rightarrow K_i q$ (Kripke Axiom: $K_i(p\rightarrow q) \rightarrow (K_i p \rightarrow K_i q)$)
				\item Menschliches Wissen ist oft inkonsistent
				\item Menschen glauben nicht alle äquivalenten Aussagen
			\end{itemize}
		\end{itemize}
	\end{itemize}
	\subsection{BDI-Logic}
	\begin{description}
		\item [Beliefs]
			\begin{itemize}
				\item \textbf{Aktuelles Weltwissen} inklusive Umgebungszustand und internem Wissen
				\item \textbf{Informationskomponente} über Systemzustand
			\end{itemize}
		\item[Desires] 
			\begin{itemize}
				\item Beschreiben den \textbf{Zustand, den ein Agent zu erreichen versucht}
				\item Kann als \textbf{Anregungszustand des Systems} betrachtet werden
			\end{itemize}
		\item[Intentions]
			\begin{itemize}
				\item \textbf{Der gewählte Weg} eines Agenten um seine Desires zu erreichen.
				\item Intention beschreibt die Komponente zur \textbf{Entscheidungsfindung} des Agenten
				\item \textbf{Committment} beschreibt Grad der Festlegung auf eine Aktionsausführung
			\end{itemize}
	\end{description}
	
	\begin{itemize}
		\item Klassische logische Operatoren: und, oder, Negation usw...
		\item CTL* Path Quantifikatoren (Computation Tree Logic)
		\begin{itemize}
			\item $A \phi$: es gilt auf allen Pfaden $\phi$
			\item $E \phi$: es gilt auf einigen Pfaden $\phi$
			\item $G$: global
			\item $F$: zukünftig
		\end{itemize}
		\item BDI-Funktionen:
		\begin{itemize}
			\item (Bel i $\phi$): i glaubt $\phi$
			\item (Des i $\phi$): i will $\phi$
			\item (Int i $\phi$): i verfolgt $\phi$
		\end{itemize}
		\item Regeln:
		\begin{itemize}
			\item (Des $\alpha$) $\rightarrow$ (Bel $\alpha$) (Belief goal compatibility): Wenn der Agent $\alpha$ erreichen will, folgt daraus, dass der Agent an die Machbarkeit von $\alpha$ glaubt.
			\item (Int $\alpha$) $\rightarrow$ (Des $\alpha$) (Goal-intention compatibility): Wenn der Agent $\alpha$ verfolgt, folgt daraus, dass der Agent $\alpha$ erreichen will.
			\item (Int does(a)) $\rightarrow$ does(a) (Volitional commitment): Wenn der Agent does(a) ausführen möchte, wird er diese als nächstes ausführen.
			\item (Des $\alpha$) $\rightarrow$ (Bel (Des $\alpha$)) und (Int $\alpha$) $\rightarrow$ (Bel (Int $\alpha$)) (Awareness of goals \& intentions): Bedingt, dass neue Ziele und Intentionen als Event gepostet werden, müssen in der Wissensbasis...
%TODO Satz vervollständigen -> müssen in der Wissensbasis....
			\item done(a) $\rightarrow$ (Bel (done(a))) (No unconscious actions): Wenn ein Agent eine Aktion ausgeführt hat, weiß er, dass er diese ausgeführt hat.
			\item (Int $\alpha$) $\rightarrow$ AF($\neg$(Int $\alpha$)) (No infinite deferral): Ein Agent will eine Intention verfolgen oder diese verwerfen.
		\end{itemize}
		\item reaktive und proaktive Agenten
		\begin{itemize}
			\item Reaktiv: Aktion wird ausgeführt durch eine Eingabe: Wenn Eingabe = p denn tu a
			\item Proaktiv: Planung und Ausführung von Aktionen, um ein Ziel zu erreichen: Tu a, um p zu erreichen
		\end{itemize}
	\end{itemize}
	
\section{Planning}
	\subsection{Einführung - Brooks Vision}
	\begin{itemize}
		\item Ziel 1: Intelligentes Verhalten ohne explizite Repräsentation von Wissen
		\item Ziel 2: Intelligentes Verhalten ohne abstraktes Schließen über die Repräsentation des Wissens
		\item Idee 1: Echte Intelligenz gibt es nur ein einer Welt und nicht losgelöst von dieser wie, in Theorem Beweisern und Expertensystemen
		\item Idee 2: Intelligentes Verhalten entsteht erst als Ergebnis der Interaktion mit der Umgebung
	\end{itemize}
	\subsection{Subsumption Architektur}
	\begin{itemize}
		\item Traditionell: Die Intelligenz steht zwischen Beobachtung und Aktion, d.h. aus Beobachtungen wird geschlossen welche Aktion ausgeführt wird.
		\item Neu: Die Intelligenz entsteht beim Beobachter durch die Aktionen in der Welt, d.h. ein Agent ist nicht an sich intelligent, sondern wirkt intelligent für einen Beobachter
		\item Entscheidungsfindung durch verschiedene Aufgaben:
		\begin{itemize}
			\item Verhalten ist eine Abbildung von Zustand auf Aktion
			\item Verarbeitung der Sensorwerte mit Schluss auf den Zustand
			\item Zustände und Aktionen sind direkt gekoppelt
		\end{itemize}
		\item Mechanismus zur Auswahl der Aktionen: Prioritäten  $\rightarrow$ d.h Verhalten mit hoher Priorität kann Verhalten mit niedrigeren Prioritäten unterdrücken
		\item Formales Modell:
		\begin{itemize}
			\item Ein Verhalten (Behavior) $b \in Beh$ ist ein Tupel $(c,a)$ mit $c \subseteq P, a \in A$ wobei P eine Menge von Beobachtungen und A eine Menge von Aktionen ist.
			\item Ein Verhalten wird ausgeführt, wenn die Umgebung sich in dem Zustand $s\in S$ befindet und $see(s) \in c$
			\item Subsumption Hierarchie wird realisiert durch eine Hemmungs-Relation $b_{1} \prec b_{2}$ ($b_{1}$ hemmt $b_{2}$ d.h. $b_{1}$ hat eine höhere Priorität)
		\end{itemize}
		\item Aktionsauswahl-Algorithmus:
		\begin{enumerate}
			\item Berechne die Menge von aktivierbaren Aktionen $FB = \{(c,a)|(c,a)\in Beh \wedge see(s)\in c\}$
			\item Für jede Aktion in FB überprüfe, ob es eine Aktion in FB gibt welche eine höhere Priorität hat
			\item Wenn keine Aktion mit höherer Priorität gefunden wurde: gib $a$ zurück; sonst: NULL (Keine Aktion gefunden)
		\end{enumerate}
		\item Vorteile:
		\begin{itemize}
			\item Einfach und hohe Ausdrucksfähigkeit
			\item Die Berechnung ist einfach nachzuvollziehen
			\item Robust gegen Ausfälle
			\item Das gesamte Verhalten entsteht durch Interaktion mit der Umwelt
		\end{itemize}
		\item Nachteile:
		\begin{itemize}
			\item Verhalten wird hart kodiert unter der Annahme die Umgebung genau zu kennen
			\item Schwierige Entscheidung über das Standardverhalten
			\item Langwierige Entscheidungen schwer möglich
			\item Skaliert nicht in größeren Systemen
		\end{itemize}
	\end{itemize}
	\subsection{Planung}
	\begin{itemize}
		\item Grundideen:
		\begin{itemize}
			\item Beschreibung des Zieles (Intention), das erreicht werden soll
			\item Beschreibung der Aktionen, die ausgeführt werden können
			\item Beschreibung der Umgebung
			\item Beschreibungen + Planer = Plan welches das Ziel erreicht
		\end{itemize}
		\item Umsetzung durch z.B STRIPS Planer
		\begin{itemize}
			\item Repräsentation der Umgebung durch Ontologie (Begriffe + Relationen)
			\item Beschreibung der aktuellen Welt durch Verwendung der Ontologie Begriffe (closed world assumption: alles was nicht angegeben wird, ist falsch)
			\item Jede Aktion hat 
				\begin{itemize}
				\item einen Namen, 
				\item eine Pre-Condition Liste (alle Bedingungen müssen wahr sein bevor Aktion ausgeführt werden kann),
				\item eine Delete-Liste (Bedingungen welche nach der Ausführung nicht mehr wahr sind), 
				\item eine Add-List (Bedingungen, die nach der Ausführung der Aktion gelten. Es können alle Variablen für allgemeine Aussagen erhalten sein.)
				\end{itemize}
			
		\end{itemize}
		\item Ein Plan ist eine Liste von Aktionen, mit Variablen ersetzt durch Konstanten. Die Ausführung der Aktionen führt vom aktuellen Zustand in einen Zustand, der das Ziel erfüllt. Der Plan ist vollständig (keine weiteren Aktionen notwendig) und konsistent (alle Pre-Conditions sind erfüllt) und die Schritte können hintereinander ausgeführt werden, ohne dass die Ausführung eines Schrittes beeinflusst wird.
		\item Formal: partially ordered Plans
		\begin{itemize}
			\item Schritte eines Plans sind mit partieller Ordnung $\prec$: $S_{i} \prec S_{j}$ bedeutet, dass $S_{i}$ vor $S_{j}$ ausgeführt werden muss
			\item Eine Menge von variablen Zuordnungen $x=t$, mit x ist eine Variable und t ist eine Konstante
			\item Eine Menge von kausalen Relationen: $S_{i} \rightarrow S_{j}$ bedeutet, dass die Ausführung $S_{i}$ die Vorbedingungen von $S_{j}$ wahr macht (impliziert  $S_{i} \prec S_{j}$)
		\end{itemize}
		\item Formale Eigenschaften: Konsistenz und Vollständigkeit
		\begin{itemize}
			\item Vollständigkeit: 
			\begin{itemize}
				\item Es gilt: $\forall S_{j}$ mit $c\in Precond(S_{j})$ und $\exists S_{i}$ mit $S_{i} \prec S_{j}$ und $c\in Effect(S_{i})$ (die Vorbedingungen für $S_{j}$ sind Teil des Effektes von $S_{i}$)
				\item Für eine Sequenz gilt:$\forall S_{k}$ mit $S_{i} \prec S_{k} \prec S_{j}, \neg c \notin Effect S_{k}$
			\end{itemize} 
			\item Konsistenz: Wenn $S_{i} \prec S_{j}$ denn $S_{j} \prec S_{i}$ und wenn $x=A$ denn $x \neq B$ für verschiedene A und B für die Variable x.
		\end{itemize}
		\item Vorlesungsfolien für Beispiel!!!
		\item Iterative Erstellung eines Plans durch rückwärts Anwenden der Regeln, d.h. in jedem Schritt wird eine offene Bedingung durch die entsprechende Aktion erfüllt.
		\item Dadurch können Konflikte entstehen:
		\begin{itemize}
			\item Ein Konflikt ist gdw. wenn $S_{3}$ die kausale Ordnung zwischen $S_{1}$ und $S_{2}$ bedroht.
			\item Lösung 1: Demotion - $S_{3}$ vor $S_{1}$ und $S_{2}$ ausführen
			\item Lösung 2: Promotion - $S_{3}$ nach $S_{1}$ und $S_{2}$ ausführen
			\item Sollte dies wieder zu Konflikte führen, Backtrack und eine andere Lösung ausprobieren in dem $S_{3}$ neben $S_{1}$ zu $S_{2}$ eingeordnet wird.
		\end{itemize}
	\end{itemize}
	\subsection{Planning Agents}
	\begin{itemize}
		\item Erster Ansatz für den Planning Agent:
		\begin{enumerate}
			\item Beobachte die Umgebung
			\item Aktualisiere das interne Modell der Umgebung
			\item Ermittle welche Intention als nächstes erreicht werden soll
			\item Benutze means-end Resoning für die Erstellung des Plans welche die Intention erreicht
			\item Führe Plan aus
		\end{enumerate}
		\item \textbf{means-end Resoning}: Gib dem Agenten eine Repräsentation:
			\begin{itemize}
			\item der Ziele/Intentionen, die erreicht werden sollen
			\item Aktionen, die er ausführen kann
			\item der Umgebung 
			\end{itemize}
		 $\rightarrow$ Der Agent nutzt die Repräsentationen, um einen Plan zu generieren.
		\item Problem: Planung und Ermittelung, welches Ziel als nächstes erreicht werden soll, kosten Zeit.
		\item Dadurch kann eine Situation entstehen, in der der Agent ein Ziel erreichen will, das nach dessen Ermittelung nicht mehr optimal ist.
		\item Unter folgenden Annahmen ist die getroffene Entscheidung noch immer optimal: Wenn Planung und  Ermittelung schnell genug sind; die Welt sich nicht verändert hat; getroffene Zielentscheidung ist noch immer optimal, wenn Agenten Plan gefunden hat;
		\item Agenten Algorithmus formal 1:
		\begin{enumerate}
			\item B = $B_{0}$ - Ausgangs-Beliefs
			\item führe unendlich lange aus:
			\item beobachte Umwelt - p
			\item B = brf(B,p) - Aktualisierung des der eigenen Beliefs
			\item I = deliberate(B) - Ermittelung
			\item $\pi$ = plan(B,I) - Plane
			\item execute($\pi$) - führe Plan aus
		\end{enumerate}
		\item Ermittelung durch: option-generation (Erstelle mögliche Ziele) und filter (Auswahl des Ziels)
		\item option-generation: Nutzt aktuelle Beliefs und Intententions und ermittelt daraus eine Menge von Optionen (Desires)
		\item filter: Der Agent wählt zwischen verschiedenen Alternativen aus den Optionen und beginnt die gewählte Option zu verfolgen	
		\item Agenten Algorithmus formal 2 - BDI Agent:
		\begin{enumerate}
			\item B = $B_{0}$ - Ausgangs-Beliefs
			\item führe unendlich lange aus:
			\item beobachte Umwelt - p
			\item B = brf(B,p) - Aktualisierung des der eigenen Beliefs
			\item D = options(B,i) - Desires (Ziele)
			\item I = filter(B,D,I) - Ermittelung der Intentionen durch Beliefs(Wissen über Umwelt), Desires(Ziele) und Intentions(gewählten Ziel)
			\item $\pi$ = plan(B,I) - Plane
			\item execute($\pi$) - führe Plan aus
		\end{enumerate}
		\item Begriffserklärungen:
		\begin{itemize}
			\item Beliefs - Weltwissen (Alles was wir wissen d.h. was wir über die Welt, unsere Fähigkeiten und Zielen glauben)
			\item Desires - Ziele (Optionen, die wir gerne erfüllt hätten)
			\item Intentions - Absichten (Ziel, die ich jetzt erreichen möchte)
		\end{itemize}
	\end{itemize}
	\subsection{Commitments}
	\begin{itemize}
		\item Strategien:
		\begin{itemize}
			\item Blind Commitment - Agent will Intention erreichen, bis er glaubt die Intention erreicht zu haben
			\item Single-minded Commitment - Agent will Intention erreichen, bis er glaubt diese erreicht zu haben oder es nicht mehr möglich ist
			\item Open-minded Commitment - Agent will Intention erreichen, bis er glaubt diese erreichen zu können
		\end{itemize}
		\item Anpassungen des Agenten Algorithmus
		\begin{enumerate}
			\item wie bisher $\rightarrow$ Blind Commitment
			\item wie bisher - Ermittle Plan und solange dieser nicht leer ist führe diesen schrittweise aus, nach jedem Schritt hole neuen Percept + Belief Update: wenn Ziel nicht mehr erreicht werden kann ermittle neuen Plan $\rightarrow$ Single-minded da keine neuen Intentionen erwogen werden
			\item wie bisher - Ermittle Plan und solange dieser nicht leer ist führe diesen Schrittweise aus solange Intention nicht erreicht und dieser noch nicht unmöglich ist, hole nach jedem Schritt neuen Percept + Belief Update + Desires ermitteln + Intentions. Wenn das Ziel nicht mehr erreicht werden kann, ermittle neuen Plan.
		\end{enumerate}
		\item Problem: Agent will Intentions verfolgen, selbst wenn schon klar ist, dass das Ziel nicht mehr erreicht werden kann. Der Agent will konstant berücksichtigten, dass evtl. unnötige Zeit verschwendet wurde und deshalb nie das Ziel erreicht werden kann.
		\item Lösung: Meta-Level Control, das entscheidet, wann der Agent seine Intention verwirft
	\end{itemize}
\section{Kooperation von Agenten}
	\subsection{Einführung}
	\begin{itemize}
		\item Ein Multi-Agenten-System besteht aus mehren Agenten, die
		\begin{itemize}
			\item durch Kommunikation interagieren
			\item mit der Umgebung interagieren können
			\item unterschiedliche Einflussbereiche auf die Umgebung haben
			\item in Beziehungen zu einander stehen (Organisation)
		\end{itemize}
		\item Egoistische Agenten bergen Potential für Konflikte da, jeder seine Ziele verfolgen möchte
		\item Lösung: Kooperation mittels Entscheidungstheorie
	\end{itemize}
	\subsection{Entscheidungstheorie - Spieltheorie}
	\begin{itemize}
		\item Wir haben zwei Agenten $A_{i}$ und $A_{j}$
		\item Jeder Agent hat die gleiche Menge $\Omega = \{\omega_{1},\omega_{2},\dots\}$ - eine Menge von Ergebnissen, die für den Agenten von Bedeutung sind.
		\item Die Bedeutung eines Ergebnisses für einen Agent i wird durch eine Utilty-Funktion repräsentiert $u_i: \Omega \rightarrow \mathbb{R}$ (jeder Agent hat eine Utility-Funktion)
		\item Utility-Funktion bildet Ordnung der Ergebnisse: $\omega \geq_{i} \omega^{'}$ bedeutet also $u_{i}(\omega) \geq u_{i}(\omega^{'})$
		\item Utility beschreibt den Nutzen, nicht das GELD!
		\item Modell für mehrere Agenten:
		\begin{itemize}
			\item Agenten wählen Aktion gleichzeitig aus, $\rightarrow$ als Ergebnis etwas aus $\Omega$
			\item Das aktuelle Ergebnis hängt von den Kombinationen der Aktionen ab
			\item Jeder Agent hat nur zwei mögliche Aktionen: C und D
			\item $\tau: A_{c} \times A_{c} \rightarrow \Omega $ eine Funktion, welche die Aktionen von den beiden Agenten auf ein Ergebnis abbildet
			\item Eine Welt lässt sich mittels $\tau$ beschreiben (unter Verwendung aller möglichen Kombinationen von Aktionen) $\rightarrow$ hat ein Ergebnis
			\item Jeder Agent kann die Utility von dem Ergebnis berechnen
		\end{itemize}
		\item Darstellung mittels Payoff Matrix (Ein Agent ist Spalte (rechter Eintrag), ein Agent ist Zeile (linker Eintrag)
		\newline
		\begin{game}{2}{2}
			   & D & C \\
			 D & 1,1 & 1,4\\
			 C & 4,1 & 4,4
		\end{game}
		\item Beispiel: $\tau(D,C) = \omega_{1}$ mit $u_{i}(\omega_{1}) = 1$ und $u_{j}(\omega_{1})=4$
	\end{itemize}
	\subsection{Strategien}
	\begin{itemize}
		\item Dominante-Strategie
		\begin{itemize}
			\item Aus einer gegebenen Strategie s (Beispiel: D oder C) für Agent $i$ ergeben sich verschiedene Ergebnisse ($\omega$)
			\item Strategie $s_{1}$ dominiert Strategie $s_{2}$, wenn jedes Ergebnis das $s_{1}$ spielt gegenüber $s_{2}$ bevorzugt wird.
			\item Rationale Agenten werden niemals dominierte Strategien spielen, d.h. es gäbe eine Strategie, die besser ist
			\item Ziel: dominierte Strategie eliminieren
			\item Anmerkung: es ist nicht immer möglich, nur eine nicht-dominierte Strategie zu finden.
		\end{itemize}
		\item Nash-Equilibrium (Nash-Gleichgewicht):
		\begin{itemize}
			\item Strategie $s_i$ und $s_j$ sind in einem Nash-Gleichgewicht gdw: Unter der Annahme, dass Agent i $s_i$ spielt und kann Agent j nicht besser sein, als wenn er $s_j$ spielt \textbf{UND} unter der Annahme, dass Agent j $s_j$ spielt und kann Agent i nicht besser sein, als wenn er $s_i$ spielt
			\item Aber: nicht jede Interaktion hat ein Nash-Gleichgewicht und einige Interaktionen haben mehrere Nash-Gleichgewichte
		\end{itemize}
		\item Pareto Optimum:
		\begin{itemize}
			\item Gegeben: eine initialen Zuordnung von Gütern für eine Menge von Agenten
			\item Eine Änderung, die für einen Agent besser ist aber für keinen anderen Agenten schlechter nennt sich Pareto-Verbesserung
			\item Eine Zuweisung ist Pareto-Optimal gdw. keine weiteren Pareto-Verbesserungen gemacht werden können
		\end{itemize}
		\item Kompetitive- und Null-Summen-Interaktionen
		\begin{itemize}
			\item Szenarien in denen Agenten gegenläufige Präferenzen haben, sind streng kompetitiv
			\item Null-Summen Spiele gdw. die Summe aller Utilitys der Agenten = 0 sind: $u_i(\omega) + u_j(\omega) = 0, \forall \omega \in \Omega$
			\item Null-Summen Spiele sind streng kompetitiv, im Real-Life selten
		\end{itemize}
		\item Beispiel: Prisoners Dilemma
		\begin{itemize}
			\item Zwei Männer wurden wegen Diebstahls verhaftet 
			\item Wenn einer gesteht und der andere nicht, geht der Schweigende 5 Jahre ins Gefängnis und der andere kommt frei
			\item Wenn beide gestehen, gehen beide für 3 Jahre ins Gefängnis
			\item Wenn keiner gesteht, geht jeder 1 Jahr ins Gefängnis
			\item Nash-Equilibirum gdw. jeder gesteht
			\item Besser wäre aber wenn jeder schweigt
		\end{itemize}
		\item Grundlegendes Problem von Multi-Agenten Interaktionen: Es wird keine Kooperation entstehen, wenn jeder Agent egoistisch ist
		\item Lösung: Annahme mein Gegenspieler ist meine Zwilling, oder Shadow of the Future (nochmaliges treffen in der Zukunft)
		\item Iterated Prison Dilemma
		\begin{itemize}
			\item Lösung: Spiele das Spiel mehrmals
%TODO Vertauscht? Es würde immer gestanden werden, um mehr Payoff rauszuholen
			\item Rückwärts Induktion: Annahme wir spielen n mal, in Runde n-1 wollen wir schweigen für höheren Payoff, dadurch wird n-2 zur letzten richtigen Runde wo wir auch schweigen würden für höheren Payoff $\dots$ Prisoners-Dilemma mit fixer Anzahl an Runden ist schweigen immer die beste Strategie
			\item Untersuchung in Axelrods Tournament: Iterated Prison Dilemma gegen verschiedene Gegner
			\item Verschiedene Strategien: ALLD (always defect), TIT-FOR-TAT (Kooperation in der ersten Runde, danach immer die Aktion welche der Gegner gespielt hat), TESTER, JOSS
		\end{itemize}
	\end{itemize}
	\subsection{Benevolent Agents - Gutmütige Agenten}
	\begin{itemize}
		\item Wenn wir das ganze System kontrollieren, können wir Agenten bauen, die sich gegenseitig unterstützen.
		\item Es gelten Konventionen über das Verhalten.
		\item Unter dieser Annahme: Unsere Agenten sind mehr oder weniger gutmütig, d.h. unser bestes Ziel ist deren bestes Ziel.
		\item Gutmütige Agenten vereinfachen das System-Design.
		\item Gutmütige Agenten haben eigene Interesse, die aber nicht mit anderen Interessen kollidieren.
	\end{itemize}

\section{Anwendung des Prisonersdilemma: Sensornetzwerk Beispiel}
\section{Agenten Kommunikation}
	\subsection{Einführung}
	\begin{itemize}
		\item Kommunikation ist der Austausch von Informationen zwischen Agenten.
		\item Für Kooperation ist es notwendig zu kommunizieren: Speech Acts, KQML \& KIF, FIPA ACL
	\end{itemize}
	\subsection{Speech Acts}
	\begin{itemize}
		\item Behandlung der Kommunikation bei Multi-Agenten-Systemen ist häufig an die Speech-Act-Theory angelegt
		\item Alles wird geäußert, um ein Ziel oder eine Intention zu erfüllen
		\item Speech Act Theorie: Wie Äußerungen genutzt werden, um Ziele zu erreichen
		\item Verschiedene Typen von Speech Acts (Searle):
		\begin{description}
			\item [Representatives:] informieren zb. \emph{Es regnet.}
			\item [Directives:] versucht den Hörer zu etwas zu bewegen zb. \emph{Bitte mach mir Tee!}
			\item [Commissives:] Sprecher verspricht etwas zu tun zb. \emph{Ich verspreche, dir einen Tee zu machen}
			\item [Expressives:] Sprecher teilt seinen mentalen Zustand mit z.b Danke
			\item [Declarations:] wie \emph{Krieg erklären} oder \emph{Taufe}
		\end{description}
		\item Jeder Speech-Act hat zwei Komponenten: \\
		performative Verb (anfordern, informieren, ...) $+$ propositional content (die Tür ist verschlossen)
		\item Communication as Action: den Hörer zu einer Aktion zu bewegen
		\begin{itemize}
			\item Versprechen: Sachen anbieten oder Sachen zu tun (Promise)
			\item Anfrage: Anderen Agenten etwas über die Gruppe fragen (Query)
			\item Bitte: Anderen Agenten bitten etwas zu tun (Request)
		\end{itemize}
		\item Probleme mit Kommunikation: Wann soll kommuniziert werden? Welcher Speech-Act ist der richtige für die aktuelle Situation? 
		\item Probleme mit Verstehen: Welche Situation kann dieses Speech-Act erzeugt haben?
		\item Komponenten der Kommunikation:
		\begin{itemize}
			\item am Sprecher
			\begin{enumerate}
				\item Intention: S möchte dass H an P glaubt
				\item Generation: S wählt Worte W
				\item Synthese: S kommuniziert die Worte W
			\end{enumerate}
			\item am Hörer
			\begin{enumerate}
				\item Perception: H empfängt $W^1$ (ideal $W^1 = W$)
				\item Analysis: H schließt aus $W^1$ mögliche Bedeutungen $P_1,\dots,P_n$
				\item Disambiguation: H schließt, dass S $P_i$ mitteilen möchte (ideal $P_i = P$)
				\item Intercorporation: H entschließt $P_i$ zu glauben (oder verwirft es, wenn es nicht mit dem aktuellen Glauben zusammen passt)
			\end{enumerate}
		\end{itemize}
		\item Modelle der Kommunikation
		\begin{itemize}
			\item \textbf{Encoded Message Model}: Sprecher kodiert die Nachricht in Wörtern, Der Hörer versucht die Nachricht zu dekodieren; Die Bedeutung der Nachricht, die übertragene Nachricht und die Interpretion sind gleich (falls keine Probleme der Kommunikation auftreten)
			\item \textbf{Situated Langage Model}: Bedeutung der Nachricht beruht nun auf er Nachricht und der Situation
		\end{itemize}
		\item Typen von kommunizierenden Agenten
		\begin{itemize}
			\item Kommunikation unter Verwendung von \textbf{Tell und Ask}: Agenten teilen sich eine gemeinsame interne Repräsentation einer Sprache; Kommunikation ohne Verwendung einer externen Sprache
			\begin{center}
				\includegraphics[scale=0.3]{img/tell and ask.png}
			\end{center}
			\item Kommunikation unter Verwendung von \textbf{Formalen Sprachen}: Agenten machen keine Annahmen über die interne Repräsentation der Sprache; Agenten teilen eine gemeinsame Sprache für die Kommunikation.
			\begin{center}
				\includegraphics[scale=0.3]{img/formalLanguage.png}
			\end{center}
		\end{itemize}
		\item Größtes Problem: Kommunikation ist zweideutig; Lösung unter Berücksichtigung des Kontextes und der vorigen Kommunikationen
		\item Definition der Semantik auf Basis von Planung: bestimmte Prädikate, die in der jeweiligen precondition-delete-add Listen verwaltet werden
		\item Beispiel: request(s,h,$\phi$)
		\begin{itemize}
			\item pre: s glaubt h kann $\phi$ tun; s glaubt h glaubt h kann $\phi$; s glaubt s will $\phi$
			\item post: h glaubt s glaubt s will $\phi$
		\end{itemize}
	\end{itemize}
	\subsection{KQML und KIF}
	\begin{itemize}
		\item Nun Betrachtung von Agent Communication Languages (ACLs) d.h. dem Standardformat für den Austausch von Nachrichten
		\item Bekannteste ACL: KQML (Knowledge Query and Manipulation Language)
		\item Zwei Bestandteile: Wissens-Abfrage und -Manipulations Sprache (KQML) und Wissens-Austausch Format (KIF)
		\item KQML ist eine äußere Sprache, die verschiedene akzeptierbare Verben (performatives) definiert (Bsp: ask-if, perform, tell, reply)
		\item KIF (knowledge interchange format)ist eine Sprache für die Repräsentation des Nachrichteninhaltes (meistens ähnlich zu Common Lisp)
		\item Für die Kommunikation müssen die Agenten das gleiche Verständnis von Ausdrücken haben (Was bedeutet Schalter1?), formale Definition in einer Ontologie
	\end{itemize}
	\subsection{FIPA}
	\begin{itemize}
		\item Neue Entwicklung der Agenten Standards incl. ACL
		\item Basisstruktur ist ähnlich zu KQML: Permative + Housekeeping + Content
		\item INFORM und REQUEST sind die zwei Standard Permative in FIPA; der Rest wird auf Basis dieser definiert
		\item Bedeutung von INFORM und REQUEST hat zwei Parts: pre-condition (muss wahr sein damit der Speech Act erfolgreich ist), rational effect (Ziel, das der Sender der Nachricht erreichen möchte)
		\item Beschreibung der pre-condtions usw als logischer Ausdruck
	\end{itemize}
	\subsection{Ontologien}
	\begin{itemize}
		\item Situation: Service Oriented Computing
		\item Alle Teilnehmer haben ein gemeinsames, gleiches Verständnis von Begriffen über einer Domäne (Ontologie) d.h. jeder Teilnehmer weiß genau, was unter einem bestimmten Begriff zu verstehen ist
		\item Eine Ontologie beschreibt Begriffe und deren Relationen zu einander
		\item Ontologien unterstützen die Interaktion. Dadurch, dass sie Begriffen eine Bedeutung geben
		\item Ontologie-Sprachen erlauben dem Nutzer eine explizite und formale Kozeptionierung der Domäne
		\item Problem: je umfangreicher die Sprache desto schwieriger wird es aus diesen zu schließen
		\item Schließen in einer Ontologie
		\begin{itemize}
			\item Klassenzugehörigkeit: Wenn x in der Klasse C und C eine Unterklasse von D, ist x auch in der Klasse D
			\item Klassenäquivalenz: Wenn Klasse A ist äquivalent zu B und B zu C denn ist A äquivalent zu C
			\item Konsistenz: Wir haben X Instanzen von A und B aber  A und B sind disjunkt d.h. Fehler in der Ontologie
			\item Klassifikation: gegeben Eigenschaft-Werte-Paare für Klasse A, wenn x die Kriterien erfüllt ist x in A
		\end{itemize}
		\item Schließen ist wichtig für: Überprüfung der Ontologie und des Wissens; Überprüfung, ob ungewollte Beziehungen der Klassen entstanden sind; automatische Zuordnung von Instanzen zu Klassen
		\item Überprüfungen sind wertvoll für: den Entwurf von großen Ontologien mit mehren Autoren; Integration und Verteilung anderer Ontologien an/von verschieden Quellen
	\end{itemize}
	\section{Wie sollten Agenten kommunizieren? Strategien und Protokolle}
	\subsection{Einführung}
	\begin{itemize}
		\item Kommunikation ist wichtig für die Verteilung von Aufgaben(Aufteilung von Teilaufgaben) und Ergebnissen (Teilergebnisse)
		\item Protokolle regeln die Interaktion zwischen Agenten
	\end{itemize}
	\subsection{Contract Net}
	\begin{itemize}
		\item Realisierung von Multi-Cast Kommunikation in Agenten-basierten Systemen
		\item Zwei Rollen: Selector und Contractor
		\item Contract Net: Aufgabenverteilungs-Protokoll für Aufgabenzuweisung
		\item Schritte:
		\begin{enumerate}
			\item Recognition: Feststellung des Problems
			\item Announcement: Mitteilung des Problems an alle Teilnehmer
			\item Bidding: Teilnehmer antworten mit den erwarteten Kosten
			\item Awarding: Auftragsgeber teilt einem Teilnehmer die Aufgabe zu
			\item Expediting: Ausführung
		\end{enumerate}
		\item Recognition
		\begin{itemize}
			\item Agent nimmt Problem wahr
			\item Agent hat Ziel und merkt, dass er das Ziel nicht alleine erreichen kann (Nicht genügend Kapazität) bzw. dass er dieses Ziel nicht alleine erreichen möchte (Deadline, Lösungsqualität)
		\end{itemize}
		\item Announcement
		\begin{itemize}
			\item Agent sendet Aufgabe incl. Spezifikation der Aufgabe an alle Teilnehmer
			\item Spezifikation beinhaltet: Aufgabenbeschreibung, Anforderungen (Deadlines, Qualität der Lösung), Meta-Informationen
			\item Announcement ist ein Broadcast
		\end{itemize}
		\item Bidding
		\begin{itemize}
			\item Agenten empfangen Announcement und entscheiden sich, ob sie für die Aufgabe bieten möchten
			\item Faktoren: Agent muss entscheiden, ob er in der Lage ist die Aufgabe zu bewältigen; Agent muss die Contraints ermitteln ggf. Preis
			\item Wenn Agent mitbieten möchte, sendet er ein Gebot (tender) an Aufgabensteller
		\end{itemize}
		\item Awarding \& Expediting
		\begin{itemize}
			\item Aufgabensteller muss aus den Geboten auswählen und entscheiden wer belohnt werden soll
			\item Ergebniss wird den teilnehmenden Agenten mitgeteilt
			\item Erfolgreicher Contractor (Teilnehmer) führt die Aufgabe aus
			\item Ggf. Erweiterung der Contractor Beziehung um weitere Unterbeziehungen: sub-contracting
		\end{itemize}
		\item Problem: Spezifikation von ...
		\begin{itemize}
			\item Aufgaben: gemeinsame Sprache der Agenten
			\item Lösungsqualität: Anforderungen (Ontologien), Werbungen und Reputation
			\item Auswahl von konkurrierenden Angeboten: Präferenzen
		\end{itemize}
	\end{itemize}
	\section{Verhandlungen}
	\subsection{Einführung}
	\begin{itemize}
		\item Wie können Agenten eine Übereinkunft treffen, wenn sie sich egoistisch verhalten?
		\item Worst-Case: Null-Summen-Spiele d.h. keine Übereinkunft möglich, ABER meistens besteht eine Möglichkeit zum Treffen einer kurzzeitigen Übereinkunft - zum allgemeinen Interesse.
		\item Fähigkeiten der Verhandlung notwendig
		\item Verhandlungen werden durch einen Mechanismus (Protokoll) gesteuert
		\item Mechanismus = Regeln für das Treffen von Agenten
		\item Mechanismus-Design: Entwicklung von Mechanismen mit bestimmten Eigenschaften
		\item Mechanismus-Design Eigenschaften:
		\begin{itemize}
			\item Konvergenz und garantierter Erfolg
			\item Maximierung des Gemeinwohles
			\item Pareto-Effizienz
			\item Individuelle Rationalität
			\item Stabilität
			\item Einfachheit
			\item Verteilung
		\end{itemize} 
	\end{itemize}
	\subsection{Verhandlung - Auktionen}
	\begin{itemize}
		\item Auktionen finden zwischen einem Auktionator und einer Menge von Bietern statt
		\item Ziel: Auktionator möchte eine Zuweisung von Gütern (Ressourcen) an einen Bieter erreichen
		\item Meistens möchte Auktionator den maximalen Preis erreichen, die Bieter den minimalen
		\item Auktions-Parameter
		\begin{itemize}
			\item Güter haben: Privaten Wert, Allgemeinen Wert, Korrelierten Wert
			\item Gewinnermittelung durch: erstes Gebot, zweites Gebot
			\item Gebote sind: für alle sichtbar (open cry); verborgen (sealed bid)
			\item Gebotsabgabe: einmalig (one shot), aufsteigend, absteigend
		\end{itemize}
	\end{itemize}
	\subsubsection{Englische Auktion}
	\begin{itemize}
		\item Höchstes Gebot gewinnt, für alle sichtbar, aufsteigend
		\item Dominante Strategie: minimale Erhöhung des höchsten Gebotes bis Obergrenze erreicht, falls diese überschritten: Rückzug
		\item Anfällig: Fluch des Gewinners (Bezahlt meistens zu viel), Lockvögel (Agent arbeitet mit Auktionator zusammen und treibt den Preis künstlich in die Höhe)
	\end{itemize}
	\subsubsection{Holländische Auktion}
	\begin{itemize}
		\item Offene Gebote, absteigend
		\item Auktionator startet mit hohen Startgebot
		\item Auktionator senkt den Preis, bis ein Agent ein Gebot zu dem Preis abgibt
		\item Gewinner: Agent mit der Preisabgabe
	\end{itemize}
	\subsubsection{First-Price Sealed-Bid Auction}
	\begin{itemize}
		\item Einmaliges verborgenes Gebot
		\item Eine Runde
		\item Bieter sendet Gebot
		\item Bieter mit höchstem Gebot gewinnt
		\item Gewinner bezahlt Preis des höchstem Gebotes
		\item Beste Strategie: biete weniger als den eigentlichen Wert
	\end{itemize}
	\subsubsection{Vickrey Auction}
	\begin{itemize}
		\item Second preis, sealed bit
		\item Gewinner mit dem höchstem Gebot zum Preis vom zweit-höchstem Gebot
		\item Beste Strategie: Preisabgabe zum wirklichen Wert
		\item Überbieten wird dominiert durch das Bieten des echten Wertes
%TODO Was willst du mir sagen??
		\item[$\rightarrow$] Wenn ein Bieter mit einem höheren Wert als die anderen gewinnt, hat dieser wohl überboten; wenn Bieter eines niedrigeren Wertes als andere Bieter antritt, hat er verloren obwohl überboten hat oder nicht
		\item Unterbieten wird dominiert durch Bieten des echten Wertes
		\item[$\rightarrow$] Wenn ein Bieter zu gering bietet verliert er; wenn ein Bieter zu hoch bietet wird er gewinnen
		\item Anfällig für antisoziales Verhalten
		\begin{itemize}
			\item Agent A beziffert den Wert bei 90\$ und weiß, dass Agent B 100\$ bieten würde.
			\item Agent A kann Zuschlag also nicht bekommen.
			\item Stattdessen bietet Agent A 99\$ um den Preis für Agent B in die Höhe zu treiben und diesem so zu schaden.
		\end{itemize}
	\end{itemize}
	\subsection{Probleme}
	\begin{itemize}
		\item Auktionen sind anfällig für Lügen vom Auktionator und Absprache von Bietern
		\item Alle Auktionen sind können durch Absprache der Bieter manipuliert werden 
		\item Ein böser Auktionator kann bei der Vickrey Auktion bezüglich es zweit höchsten Gebots lügen
		\item Shills (Lockvögel) können bei der Englischen Auktion den Preis in die Höhe treiben
		\item Anwendung von Auktionen: Lastverteilung, Routing, Koordination
	\end{itemize}
	\subsection{Heterogene und selbst-motivierende Agenten}
	\begin{itemize}
		\item Kein zentrales Design
		\item Es gibt keinen globalen Nutzen
		\item Sind Dynamisch d.h. neue Typen von Agenten können leicht hinzugefügt werden
		\item Agenten sind nicht wohlwollend solange sie es nicht sein wollen d.h. Agenten kooperieren erst, wenn es nötig ist
		\item Notwendig: Entwickler einigen sich auf Standards, wie Agenten in Domäne zu agieren haben
		\item Abstimmung von Möglichkeiten und Tradeoffs für Protokolle, Strategien und die sozialen Regeln der Agenten
		\item Eigenschaften von Standards:
		\begin{itemize}
			\item Effizienz: Pareto Optimal
			\item Stabil: kein Grund vom optimal abzuweichen
			\item Einfach: gerade Berechnungs- und Kommunikationskosten
			\item Verteilt: kein zentraler Koordinator
			\item Symmetrisch: jeder Agent spielt äquivalente Rolle
		\end{itemize}
		\item MAS: Gruppe von Nutzen-maximierende heterogenen Agenten, die koexistieren in der gleichen Umgebung, evtl. Konkurrenz
		\item Auktionen als Verhandlungsmechanismus für die Teilung von Ressourcen
		\item In Abhängigkeit von Auktion wählt jeder Agent eine andere Strategie
		\item Möglich: andere Szenarien für Verhandlungen außer die Verteilung von Ressourcen
	\end{itemize}
	\subsection{Verschiedene Domänen}
	\begin{itemize}
		\item Aufgaben orientierte Domäne: Agenten wollen Aufgaben erledigen $\rightarrow$ Aufgabenverteilung
		\item Zustand orientierte Domäne: Ziele sind bestimmte finale Zustände der Welt $\rightarrow$ gemeinsamen Plan
		\item Wert orientierte Domäne: Zustände haben einen Wert $\rightarrow$ gemeinsamen Plan- und Ziel-Realisierung
	\end{itemize}
	\subsection{Aufgaben orientierte Domäne}
	\begin{itemize}
		\item Tripe: $<T,Ag,c>$
		\item T ist eine endliche Menge von möglichen Aufgaben
		\item $Ag=\{1,\dots,n\}$ ist eine Menge von beteiligten Agenten
		\item $c = p(T) \rightarrow R^+$ definiert die Kosten für die Ausführung jeder Teilmenge von Aufgaben (p ist Potenzmenge)
		\item Ein Encounter ist eine Menge von Aufgaben: $<T_1,\dots,T_n>$ mit $T_i \subseteq T$ für jedes $i \in Ag$
		\item Bestandteile: Domäne, Verhandlungsprotokoll, Verhandlungsstrategie
		\item Gegeben ein Encounter, dann ist $<T_1,T_2>$ ein Deal für die Zuweisung $T_1 \cup T_2$ bezüglich der Agenten 1 und 2
		\item Die Kosten für den Agenten i eines Deals: $\beta = <D_1,D_2>$ ist $c(D_i)$ (nachfolgend als $cost_i(\beta)$)
		\item Utility eines Deals $\beta$ des Agenten i ist mit $c(T_i)$ die ursprüngliche Zuweisung einer Aufgabe für Agent i: $utility_i(\beta) = c(T_i)-cost_i(\beta)$
		\item Conflict deal $\Omega$ ist ein Deal $<T_1,T_2>$ mit der ursprünglichen Zuweisung an Aufgaben: $utility_i(\Omega) = 0, \forall i \in Ag$
		\item Ein Deal ist individuell rational, wenn dieser den conflict deal schwach dominiert
		\item Agenten nutzen Produkt-maximierende Verhandlungsprotokolle
		\item The Monotonic Concession Protocol ???
		\begin{itemize}
			\item Verhandlung in Runden
			\item Runde 1: Agenten schlagen gleichzeitig einen Deal aus dem Verhandlungsset vor
			\item Übereinkunft, wenn ein Agent einen Deal findet, der mindestens so gut ist wie die von anderen vorgeschlagenen
			\item Wenn keine Übereinkunft nächste Runde
			\item In Runde $u+1$ darf kein Agent einen schlechteren Deal machen als der in der vorigen Runde
			\item Wenn nach einigen Runden keine Übereinkunft gefunden wird: Dann wird die Verhandlung mit dem Conflict-Deal beendet
		\end{itemize}
		\item Zeuthen Strategie
		\begin{itemize}
			\item \textbf{Was sollte der erste Vorschlag sein?} | der beste Deal aus Sicht des Agenten
			\item \textbf{Welcher Agent sollte ein Zugeständnis machen?} | \begin{itemize}
				\item Agent mit der kleinsten Risikobereitschaft bezüglich Konflikt (der, der am meisten bei einem Konflikt zu verlieren hat)
				\item $r_i = \frac{\text{Nutzenminderung durch Zugeständnis}}{\text{Nutzenminderung durch Konflikt}} = \frac{u_i(d_i) - u_i(d_j)}{u_i(d_i)}$
			\end{itemize}
			\item \textbf{Wie groß sollte das Zugeständnis sein?} | gerade groß genug, so dass er beim nächsten Mal nicht der mit kleinster Risikobereitschaft ist
			\item Die Zeuthen Strategie ist ein \textbf{Nash-Gleichgewicht}: Wenn ein Agent ihr folgt, kann der andere nicht besser handeln als ihr auch zu folgen.
		\end{itemize}
	\end{itemize}
	\section{Trust and Reputation}
	\subsection{Einführung}
	\begin{itemize}
		\item Trust: Der Glaube, dass ein anderer Agent eine Aktion tun wird, \textbf{ohne} dass \textbf{explizite Garantien} vergeben werden, um ein Ziel in einer riskanten Situation zu erreichen.
		\item Reputation: Was andere Agenten bezüglich seines Verhaltens sagen; \emph{opinion  or  view  of  one about  something} $\rightarrow$ global.
		\item Reputation erlaubt die Bildung von Trust und hat so eine soziale Komponente, die nicht nur für einen Agenten sondern für alle nützlich ist. Um Reputation aufzubauen ist Kommunikation notwendig und ähnlich wie Ontologien erleichtert diese den Alltag.
		\item Jeder Agent wird durch andere Agenten beobachtet, keine zentrale Autorität
		\item Trust und Reputationsmechanismen basieren auf den zwei Layern: Sicherheit (Integrität, etc.) und Institution (Organisationsaspekte wie Abläufe oder Protokolle)
		\item Subjective vs. Globale Reputation
		\begin{itemize}
			\item Subjektiv: Jeder Agent hat eine eigene Vorstellung von der Reputation anderer Agenten
			\item Global: Reputation als eine zentrale Ressource, auf die alle Agenten Zugriff haben und die gleichen Reputationswerte erhalten.
			\item Vorteil global: Reputation ist auch für neue Agenten verfügbar; einfacher für Agenten, da diese Werte nicht berechnet werden müssen
			\item Nachteil global: Funktioniert nur unter der Annahme, dass alle Agenten denken und sich ähnlich verhalten, da interne Zustände des Agenten nicht berücksichtigt werden; Es ist nicht immer erwünscht, dass Agenten Information public machen oder diese an eine zentrale Verwaltungsstelle senden
			\item Ein hoher Trust in die zentralen Organisation ist notwendig
		\end{itemize}
		\item Vgl. Ebay: komplett zentral, Käufer hinterlassen Kommentare + Bewertung nach Käufen, jeder Teilnehmer hat einen Wert als Summe der Bewertungen, viele Nutzer, einfacher Identitätenwechsel
		\item Beispiel: Regret System
	\end{itemize}
	\subsection{Regret System}
	\begin{center}
		\includegraphics[scale=0.3]{img/regret.png}
	\end{center}
	\begin{itemize}
		\item Modulares Trust und Reputations-System für komplexe E-Commerce Umgebungen, in denen soziale Relationen zwischen den Teilnehmern eine wichtige Rolle spielen
		\item Es kann auch auf andere Eigenschaften als Trust/Reputation ankommen bspw. auf Aspekte wie Qualität oder schnelle Lieferung
		\item Die Folien und die beiden Papers beschreiben die Ansätze teilweise ziemlich unterschiedlich und widersprechen sich zum Teil sogar:
		\begin{itemize}
				\item Anschauliches Paper: \url{http://www.iiia.csic.es/files/pdfs/591.pdf}
				\item Mathematisches Paper:  \url{http://www.iiia.csic.es/~sierra/articles/2001/reputation.pdf}
		\end{itemize}
	
		\item Agenten stehen in Relationen zueinander, z.B.
		\begin{description}
			\item[Competition]| Agenten verfolgen dasselbe Ziel und benötigen gleiche Ressourcen.
			Es wird versucht, gegenüber dem Anderen einen Vorteil zu gewinnen, beispielsweise durch Lügen oder das Verstecken von Informationen.
			\item[Cooperation]| Agenten haben das Interesse, zusammenzuarbeiten.
			Daher wird oft angenommen, dass der Andere vertrauenswürdig ist.
			Schließt sich mit \emph{Competition} aus
			\item[Trade]| Es existieren kommerzielle Transaktionen.
			Kompatibel mit \emph{Competition} und \emph{Cooperation}
		\end{description}
		\item Jeder Agent kennt zu jeder Relation ein \textbf{Soziogramm}, also einen gewichteten Graphen mit Agenten als Knoten und Kanten, wenn eine entsprechende Beziehung besteht.
		Die Gewichte der Kanten geben an, wie stark die jeweilige Relation (vermutlich) ausgeprägt ist ($w\in [0,1]$).
		\item \textbf{Kernkonzept:} Regret unterscheidet bei der Reputation zwischen der \textbf{Individuellen Dimension}, der \textbf{sozialen Dimension} und der \textbf{ontologischen Dimension}.
	\end{itemize}
	\subsubsection{Ontologische Dimension}
	Statt \emph{der} Reputation eines Agenten, gibt es mehrere Typen der Reputation.
	So kann ein Agent die Reputation haben, ein \emph{Schwindler} zu sein oder \emph{häufig die Qualitätsansprüche zu übertreffen}. 
	Diese Typen können auch zusammengesetzt sein.
	Ein Agent, der \emph{nicht zahlt} fällt ebenso in die Kategorie \emph{Schwindler}, wie einer der \emph{nicht liefert, obwohl er bezahlt wurde}.
	\subsubsection{Individuelle Dimension}
	\begin{itemize}
		\item Modelliert die Reputation aus der direkten Interaktion zwischen zwei Agenten.
		Dies ist die beste Methode, um eine Reputation zu einem anderen Agenten zu berechnen, ist jedoch nicht immer möglich, wenn keine oder nicht genügend Interaktionen stattgefunden haben.
		\item Beide Papers scheinen unterschiedliche Ansichten zu haben, wie man auf die individuelle Reputation kommt.
		\item Agenten speichern \textbf{Outcomes}, also Paare von Verträgen und den dazugehörigen tatsächlichen Ergebnissen in einer \textbf{Outcome Database (ODB)}.
		\item Outcomes werden vom Agenten bezüglich bestimmter Eigenschaften (z.B. Lieferzeit eingehalten, Qualitätsansprüche erfüllt etc.) bewertet.
		Beide Paper beschreiben hierfür unterschiedliche Ansätze(Impressions, Issues und Grounding Relation).
		\textbf{Hinweis}: Die im \emph{mathematischen Paper} genutzte \textbf{Impression Database (IDB)} steht im Widerspruch zur IDB in der Grafik aus den Folien!
		\item Die Reputation eines Agenten bezüglich bestimmter Eigenschaften ist (vereinfacht) die Summe der relevanten Bewertungen von Outcomes.
		\item Die \textbf{Verlässlichkeit} dieser Reputation ist Abhängig von der \emph{Anzahl} und der \emph{Streuung} der individuellen Erfahrungen.
	\end{itemize}
	\subsubsection{Social Dimension}
	\begin{itemize}
		\item Wenn keine (oder nicht genügend) eigene Erfahrungen verfügbar sind, um eine Reputation zu einem Agenten zu berechnen, muss das soziale Umfeld befragt werden.
		Da dieses sehr groß sein kann, muss eine Teilmenge aller Agenten ermittelt werden, die befragt wird. Kurzfassung:
		\begin{description}
			\item[Witness Reputation]| Befragen von Agenten, die idealerweise schon Kontakt mit dem Agenten hatten und damit schon eine Reputation über diesen kennen.
			\begin{itemize}
				\item Zeugen können lügen oder Informationen auslassen, um dem anfragenden Agenten zu schaden oder den Zielagenten zu schützen.
				\item Weiterhin tritt \emph{correlated evidence} auf, weil Agenten aufgrund derselben Ereignisse bewerten und sich gegenseitig beeinflussen.
				\item Es müssen weiterhin nicht alle Zeugen befragt werden.
				Ein vorgeschlagener (stark vereinfachter) Algorithmus zum Finden von geeigneten Zeugen nimmt ein Soziogramm aller Zeugen, identifiziert die zusammenhängenden Komponenten und wählt aus diesen jeweils die Cut-Points (Knoten, deren Wegnahme die Komponentenzahl erhöht) und, wenn in einer Komponente keine Cut-Points vorhanden sind, die Knoten mit der höchsten Gradzahl (Central Points).
				\item Das Vertrauen in den Zeugen spielt eine Rolle und muss ermittelt werden.
				Hierbei gibt es \textbf{social trust} und \textbf{outcome trust reputation}.
				\begin{itemize}
					\item Beim social trust werden die Beziehungen zwischen anfragenden Agenten, Ziel und Zeuge betrachtet und anhand von Fuzzy-Logik bewerter: \texttt{IF $coop(w_i,b)$ is high THEN socialTrust is very\_bad}
					\item Die bessere Alternative, sofern möglich, ist die Bewertung des Zeugen anhand eigener Outcomes mit diesem.
				\end{itemize}
			\end{itemize}
			\item[Neighbourhood Reputation]| ?
			\item[System Reputation]| Standardwert anhand der Rolle des Agenten, Vorurteile (z.B. \emph{Studenten kommen zu spät})
		\end{description}
	\end{itemize}
	\subsection{Fuzzy-Logik}
	Kommt dann ins Spiel, wenn etwas unscharf ist. Fuzzy Logik ist eine Theorie, welche vor allem für die Modellierung von Unsicherheit und Vagheit von umgangssprachlichen Beschreibungen entwickelt wurde. In der Vorlesung wurde als Beispiel die Einteilung in große bzw. kleine Körpergröße gemacht. Es ist unklar ab wann jemand groß ist und wann klein.\\
	
	Fuzzy Logik tritt dann auf, wenn eine Einschätzung unklar/verwischt/verschwommen/unbestimmt ist. \\
	Gegenbeispiel: Tot oder lebendig
	
	\section{Blackboard}
	Hilfreiche Papers: 
	\begin{itemize}
		\item \url{http://mas.cs.umass.edu/pub/paper_detail.php/218} (im Skript angegeben)
		\item \url{http://www.aaai.org/ojs/index.php/aimagazine/article/view/537} (yay, Koalas!)
	\end{itemize}
	\subsection{Die Blackboard Metapher}
	Steht so nicht in den Folien, jedoch in der Zusammenfassung der Folien und sollte ausreichend sein, um die Grundidee ausführlicher zu beschreiben.
	Ein \textbf{Blackboardsystem} ist durch drei Hauptkomponenten charakterisiert:
	Eine Gruppe von Experten (\textbf{Knowledge Sources | KS}), einer gemeinsam verwendeten Tafel mit (teilweise gelösten) Problemen (\textbf{Blackboard | BB}) und einem Moderator (\textbf{Controller | C}).
	 
	Es kann viele, unterschiedliche, unabhängige und asynchron arbeitende KS geben. Die KS sollen ein definiertes Problem lösen. Dazu suchen sie die Tafel nach für sie lösbaren Problemen ab, lösen diese und schreiben die Lösung an die Tafel.\\
	Corkill beschreibt folgende Charakteristiken:
	\begin{description}
		\item[Independence of expertise] (I think, therefore I am.) | KS arbeiten eigenständig und sind daher voneinander unabhängig.
		\item[Diversity in problem-solving techniques] (I don’t think like you do.) | KS sind Spezialisten auf ihren Gebieten.
		Das Problem wird in verschiedene Fachgebiete unterteilt (z.B. Fahrzeugbau $\rightarrow$ Maschinenbauingenieure, Elektroingenieure....) und die KS sollten entsprechend gewählt sein.
		\item[Flexible representation of blackboard information] (If you can draw it, I can use it.) 
		\item[Common interaction language] (What’d you say?) | Gemeinsame Sprache ist verfügbar, was ein KS auf das BB schreibt, kann von einem anderen KS verstanden werden.
		Mitunter kann es vorkommen, dass eine Sprache nur von einer Teilmenge von KS verstanden wird.
		\item[Positioning metrics] (You could look it up.) | Inhalt des BB kann von den KS effizient erfasst werden.
		So können etwa Bereiche des BB thematisch aufgeteilt sein, sodass eine KS nur einen bestimmten Bereich beobachten muss.
		\item[Event-based activation] (Is anybody there?) | KS können Ereignisse (sowohl auf BB, als auch extern) wahrnehmen und darauf reagieren.	
		\item[Need for control] (It’s my turn.) | Es wird eine Kontrollinstanz C benötigt.
		Diese lenkt die Aktionen der KS, damit das Ziel letztendlich erreicht werden kann.
		\item[Incremental solution generation] (Step by step, inch by inch...) | KS nehmen ein Problem vom BB, lösen es und schreiben die Lösung auf das BB zurück.
		Die Lösung wird von anderen KS als Input verwendet.
		So wird die Lösung an der Tafel inkrementell verbessert, bis letztendlich die Lösung des großen Problems an der Tafel steht.
	\end{description}
	\subsection{Model of Problem Solving}
	\begin{description}
		\item[Knowledge Sources] Unabhängige Systeme zum Lösen von Problemen aus bestimmten Domänen.
		Verstehen den aktuellen Zustand im Problem-Lösungsprozess und die relevanten Informationen am BB.
		Kennen außerdem die Vorbedingungen für ihre möglichen Aktionen (\emph{Was muss an der Tafel stehen, damit ich aktiv werden kann?})
		\item[Blackboard] Globale Datenstruktur, die für alle KS verfügbar ist.
		Auf ihr werden Rohdaten, Teillösungen, Alternativen, Endlösungen uns Steuerinformationen festgehalten.
		Dadurch ermöglicht das BB Kommunikation und Aktivierung von KS.
		Kann einen Flaschenhals darstellten.
		\item[Control Component] Entscheidet, welche KS auf Veränderungen auf dem BB reagieren dürfen und lenkt so die Lösungsfindung.
	\end{description}
	\subsection{Aus den Folien gekratztes Wissen}
	\begin{itemize}
		\item Kommunikation über ein gemeinsames Medium hatten wir vorher schon einmal: Ameisen
		\item Schema zum kooperativen Problemlösen über eine gemeinsame Datenstruktur\\
		Im Gegensatz zur direkten Kommunikation via *casts und Protokolle, sind Blackboard-Systeme eine indirekte Kommunikationsform.
		\item Merkmale
			\begin{itemize}
			\item BB können als Datenspeicher, Experten Systeme oder als Programmierumgebung genutzt werden
			\item Agenten können auf dem BB Teillösungen schreiben/lesen, da es eine geteilte Datenstruktur ist. Es werden Teillösungen auf das BB geschrieben.
			\item Da kein zeitgleicher Zugriff möglich ist, ist gegenseitiger Ausschluss notwendig.
			\item Unnötige Datenduplikationen werden vermieden und es kann entschieden wer die Kontrolle über das BB hat.
			\item Eine Kommunikation der KS (mit Hinblick auf Daten) erfolgt via BB.
			\item Nachteil ist jedoch, dass das BB sich zu einem Flaschenhals entwickeln kann
			\end{itemize}
		\item Subscribe/Notify Pattern
			\begin{itemize}
			\item Um Ergebnisse mit anderen Agenten zu teilen wird dieses OO-Pattern verwendet.
			\item Ein Objekt kann sich bei einem anderen anmelden, um über ein Ereignis informiert zu werden.
			\end{itemize}
		\item Grundsätzlich ist ein BB eine multidimensional Datenstruktur. Es ist aus Knoten aufgebaut, die wiederum Datenfelder beinhalten.
		\item Die Kooperation erfolgt durch \textbf{Hypothese-und-Test}: Teillösungen sind die Hypothesen, die anschließend getestet werden.
		\item Alternativen für das Problem
			\begin{itemize}
			\item Pipe and Filter
				\begin{itemize}
				\item Filter benötigen kein Wissen darüber, an was sie angebunden sind.
				\item Sie können parallel laufen
				\item Das Verhalten des Systems basiert auf dem Verhalten der einzelnen Filter
				\end{itemize}
			\item Direkte Kommunikation ist erheblich aufwändiger und führt zu:
			\item Objektorientierte Architektur
				\begin{itemize}
				\item Interessante Eigenschaften: Data Hiding (Datenrepräsentation für Clients nicht einsehbar), Wahl zwischen single- und multi-threaded und Probleme lassen sich zerlegen und an eine Menge von Agenten delegieren
				\item ABER miteinander agierende Objekte müssen sich gegenseitig kennen
				\item Lässt sich verfeinern zu Client/Server/Brocker $\rightarrow$ Schnittstelle/Middleware
				\end{itemize}
			\item Layered System: Wenn die Struktur es zulässt. Durch verschiedene voneinander gelöste Layer, kann der Designprozess, die Wiederverwendung und weitere Verbesserungen unterstützt werden. Es können Interfaces standardisiert werden. Aber es können nicht immer klare Layer erkannt werden.
			\item Event-Based
				\begin{itemize}
				\item Die Sender von Events müssen sich nicht damit auseinandersetzen, wer damit wie umgeht. Es ermöglicht eine einfachere Wiederverwendung und Weiterentwicklung des Systems - einfaches Hinzufügen von weiteren Agenten.
				\item ABER: Die Komponenten haben keine Kontrolle über das Ordnen der Berechnungen.
				\end{itemize}
			\item Model View Controller: Es gibt ein zentrales Model und viele Views. Zu jedem View gehört ein Controller, der die Eingaben des Nutzers auf dem View an das Model updatet. Änderungen am Model werden an alle Views verteilt.
			\end{itemize}
		\item Hearsay II
			\begin{itemize}
			\item Eins der ersten akademischen BB-Systeme zur Spracherkennung. 
			\item Da viele Team parallel zu einander gearbeitet haben, sollen diese partiellen Lösungen auf dem BB arbeiten und daraus eine globale Lösung generieren. So entwickelte sich ein umfangreiches System zur Spracherkennung.
			\item Es ist kein Expertensystem, das alles kann - dafür die Subsysteme.
			\end{itemize}
		\item Sinnvolle Anwendung von BB
			\begin{itemize}
			\item Entkopplung: Bspw. Sensoren in einem Raum
			\item Mehere Akteure in einer Welt: So können Bedingungen global gespeichert werden, wenn sie mehrere betreffen
			\item Gut zur Koordination zwischen Agenten $\rightarrow$ NOLF2 Beispiel
			\end{itemize}
		\item Probleme von verteilten Netzwerken: Wie oft muss aktualisiert werden? Mit wie viel falschen Wissen kann noch gearbeitet werden?
		\item Probleme bei der Koordination zwischen Agenten:
			\begin{itemize}
			\item Agenten machen Sachen gleichzeitig
			\item Agenten machen Sachen zu oft
			\item Spezielle Bedingungen für Taktiken
			\item Agenten nehmen die gleichen Wege\\
			\textit{Lösung: Rasterung/Einteilung des Gebietes. A erhöht Kosten, wo er langläuft und verhindert so dass B dort auch lang läuft.s}
			\item Agents clump at destinations
			\end{itemize}
	\end{itemize}
	\section{Organisationskontrolle}
	\subsection{Allgemein}
	\begin{itemize}
		\item Organisationskontrolle (Wer kann mit wem interagieren) kann Kommunikation und Kooperation vereinfachen (kürzere Entscheidungshierarchie, weniger Kommunikation)
		\item Strukturierung als zentrale Aufgabe reflektiert die Aufgabe und sorgt für eine effizientere Lösung der Aufgabe
		\item Organisationskontrolle funktioniert, wenn ein Problem zerlegbar und häufig wiederholbar ist
		\item Framework für die Skalierung der Aufgaben
		\item Ziel: Wissen lokal zu halten, damit nicht zusätzlicher Aufwand durch Verwaltung fremden Wissens entsteht
		\item Organisations Tradeoff
		\item Die Rollenzuteilung zur Steigerung der Effizienz kann von der Natur abgeschaut werden. Bspw. das Flugkonzept von Vögeln $\rightarrow$ Arbeitslastverteilung
	\end{itemize}
	\subsection{Selbstorganisierende Rollenänderungen}
		\begin{itemize}
		\item Beim Senken-organisierten Verfahren pollt die Senke regelmäßig die Knoten und übernimmt die Zuweisung von Aufgaben
		\item Bei der Selbstorganisation benachrichtigt ein Knoten den Clusterhead, sobald sein Energielevel unter einen bestimmten Wert gefallen ist. Dieser verteilt anschließend die Aufgabe an Knoten mit einem höheren Energielevel.
		\item Ergebnis: Erhöhrung der Lebenserwartung um 40\%\\
		(Limitierte Faktoren sind Senden, Messen und Rechnen)
			\end{itemize}
	\subsection{Kontrolle von Distributed Sensor Networks)}
		\begin{itemize}
		\item Wenn bei vielen Sensoren/Agenten keine Organisation dahintersteht, dann kann viel via Broadcasts gesendet werden; Wer entscheidet, ob ein Ziel neu ist?; Wer verfolgt das Ziel? $\rightarrow$ Diese Entscheidungen können zwar individuell getroffen werden, sind aber durch eine globale Organisation einfacher.
		\item Rollenzuteilung der Knoten: Sensor, Sector-Manager, Track Manager
		\item Die Umgebung wird in Sektoren eingeteilt und je Sektor wird ein Sektor-Manager bestimmt. Die anderen Knoten eines Sektors senden ihre Kapazitäten an den Sektor-Manager, der daraufhin Scan-Pläne entwickelt.
		\item Knoten die für das Scannen eingeplant sind, machen selbiges und teilen dem Sektor-Manager Beobachtungen mit. Es wird ein Track-Manager bestimmt.
		\item Die Aufgabe des Track-Manager ist es die Tracking-Nodes zu finden und zu koordinieren. Es kann hierbei zu Konflikten mit anderen Aufgaben kommen. Der Track-Manager sollte ab einem bestimmten Punkt in einen anderen Sektor migrieren.
		\end{itemize}
	\subsection{Content-Based Hierarchical Agent Organizations}
		\begin{itemize}
		\item Agenten mit ähnlichem Inhalt werden gruppiert. Dabei ist die Größe der zu durchsuchenden Teilgruppen limitiert. Es werden Querverweise gesetzt, um schnell Inhalt zu finden.
		\item Die Organisation vergrößert sich inkrementell, wenn neue Agenten joinen.
		\item Die Suche läuft in 2 Phasen ab: Cluster mit gesuchten Inhalt lokalisieren $\rightarrow$ Cluster durchsuchen
		\end{itemize}
	\subsection{Lane-Protokoll}
		\begin{center}
		\includegraphics[scale=0.5]{img/lane.png}
		\end{center}
		\begin{itemize}
		\item ???
		\end{itemize}
		Um \textbf{Fehlverhalten} zu analysieren, muss zuerst definiert werden, was ein Fehlverhalten ist. Außerdem kann Fehlverhalten nach schwere des Vergehens klassifiziert werden. Als nächstes muss das Fehlverhalten detektiert werden - dies verursacht jedoch Aufwand, der vermieden werden sollte.\\
		Detektion:
			\begin{enumerate}
			\item Identifizierung von kritischen Nachrichten
			\item angemessene Reaktionen überprüfen
			\item Trust-Wert anpassen
			\end{enumerate}
	\subsection{How to Create an Organization}
		\begin{itemize}
		\item Top-Down \textbf{oder} Emergent/Self-Organizing \textbf{oder} \item Kombination
		\item Typen von Agenten (kooperativ, eigennützig, semi-kooperativ)
		\item Rollen (Gruppen, Regeln, Rollen)
		\end{itemize}
	\section{Agenten und Mobilität}	
	\begin{itemize}
		\item Rückblick - Gebiete zusammengefasst
			\begin{itemize}
			\item KI (Planung, Kommunikation, Wissen, Logik)
			\item Soziologie (Wie Menschen sich verhalten)
			\item \textbf{Jetzt:} Verteilte Systeme (unabhängig, own Thread of Computiation, Garbage Collection in MAS nicht möglich, distributed Location)
			\end{itemize}			
		\item Verteilte Orte: Wie können diese miteinander reden?
		\item Verteilte Systeme: verschiedene Möglichkeiten um Methoden von anderen Koten (Remote Procedure Call) aufzurufen
		\item Lösungen: Stub, Skeleton. Marshalling (Serialisierung von Objekten), IDL
		\item Marshalling: Serialisieren, sodass darauf referenziert werden kann
		\item Stub hat Referenz zu Skeleton-Methodes
		\item Probleme: 
		\begin{itemize}
			\item Client kann blockieren
			\item Verbindung kann nicht sicher sein (fällt ggf aus), Implementierung von Timeouts usw (unreliable links)
			\item Übertragung von großen Daten kann ein Problem sein. Es ist besser, wenn der Server Berechnungen mit Daten vornimmt - ohne zu verschicken ABER möglicherweise kennt der Server den Algorithmus nicht (Lösung: Mobile Agents)
		\end{itemize}
		\item Agenten-basierter Ansatz: Alternative zu RPC ist RP (remote programming). Der Client macht hierfür Methoden und Klassen verfügbar. Die Methode wird zusammen mit dem Zustand des Agenten auf dem Server ausgeführt. (?)
		\item Client macht Code für Prozeduren und Klassen bekannt
		\item Prozedur + Aktuellen Zustand präsentiert einen Mobiler Agent
		\item Weak migration: ist heutzutage Standard\\
		(Code\&Date werden migriert. Es ist notwendig einen Startpunkt zu definieren und es ist komplizierter damit zu arbeiten)
		\item Strong migration\\
		(Code\&Data\&State(Programming Stack) Das Programm kehrt zu einem Interrupt-Point zurück. Da ein Agentenzustand groß sein kann, ist dies ein langsamer und teurer Prozess)
	
		\item Remote Programming: Konventionen zwischen Client und Server für Befehle und Datentypen bilden eine Sprache
		\item Vorteile von mobilen Agenten: Reduzierung der Netzwerklast, Offline-Ausführung von Agenten möglich, Höhere Clientflexibilität, Bessere Lastverteilung bei Berechnungen
		\item Notwendige Infrastruktur: transportierbare Sprache in der die Agenten entwickelt wurden (Java), Interpreter, Kommunikation zum Agentenaustausch, Agenten
		\item \textbf{Gründe für Java:} Plattformunabhängigkeit, Mechanismus zum Laden von Klassen, Multi-Threading, Serialisierung, RMI, Reflektion \textbf{ABER} keine strong migration
		\item Migration darf nicht im Selbstmord enden: Daher wird ein "`Shadow"' hinterlassen. Außerdem müssen Nachrichten an die neue Position weitergeleitet werden (neue Location wird in der alten gespeichert). Erst nach erfolgreichen Migration wird das alte Objekt gelöscht, im Fehlerfall wird es re-installiert		
		\item Agenten können reflektieren, wer sie zu was auffordert - Objekte können das nicht
		\item Agenten müssen selbstständig festlegen, wann sie garbage collected werden können.
		\item Sicherheitsaspekte -> Authentizität (Spoofing), Integrität(Veränderungen), DOS, ...\\
		$\rightarrow$ Authentifikation (User, Host, Agent, Code), Access Control, Vertraulichkeit
		\item Hostsicherung: Safe Code Interpretation (interpretierte Sprachen), Authentifizierung, Authorisierung, Ressourcen Allocation, Path-History
		\item Mobile-Agenten-Sicherung: safe Environment, Signaturen, shared Secret
	\end{itemize}
\end{document}